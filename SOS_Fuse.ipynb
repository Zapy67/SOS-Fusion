{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/Xtra-Computing/FedOV","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T09:59:58.370358Z","iopub.execute_input":"2025-12-28T09:59:58.370636Z","iopub.status.idle":"2025-12-28T09:59:59.228507Z","shell.execute_reply.started":"2025-12-28T09:59:58.370613Z","shell.execute_reply":"2025-12-28T09:59:59.227593Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'FedOV'...\nremote: Enumerating objects: 175, done.\u001b[K\nremote: Counting objects: 100% (158/158), done.\u001b[K\nremote: Compressing objects: 100% (68/68), done.\u001b[K\nremote: Total 175 (delta 90), reused 150 (delta 86), pack-reused 17 (from 1)\u001b[K\nReceiving objects: 100% (175/175), 95.03 KiB | 6.33 MiB/s, done.\nResolving deltas: 100% (92/92), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"cd FedOV","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T10:00:00.733615Z","iopub.execute_input":"2025-12-28T10:00:00.734215Z","iopub.status.idle":"2025-12-28T10:00:00.739436Z","shell.execute_reply.started":"2025-12-28T10:00:00.734183Z","shell.execute_reply":"2025-12-28T10:00:00.738670Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/FedOV\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile experiments.py\nimport numpy as np\nimport json\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport torch.utils.data as data\nimport argparse\nimport logging\nimport os\nimport copy\nfrom math import *\nimport random\nimport copy\nfrom PIL import Image\nfrom cutpaste import *\n\nimport datetime\n#from torch.utils.tensorboard import SummaryWriter\n\nfrom model import *\nfrom utils import *\nfrom vggmodel import *\nfrom resnetcifar import *\nfrom attack import *\n\nCLASSIFIER_EPOCHS = 5\nGENERATIVE_EPOCHS = 1\nBATCH_SIZE = 64\nLATENT_SIZE = 20\nNUM_CLASSES = 10\n\nclass Classifier(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, num_classes)\n        #self.cuda()\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return x\n\nclass Encoder(nn.Module):\n    def __init__(self, latent_size):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, latent_size)\n        #self.cuda()\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        x = norm(x)\n        return x\n\n\n# Project to the unit sphere\ndef norm(x):\n    norm = torch.norm(x, p=2, dim=1)\n    x = x / (norm.expand(1, -1).t() + .0001)\n    return x\n\n\nclass Generator(nn.Module):\n    def __init__(self, latent_size):\n        super().__init__()\n        self.fc1 = nn.Linear(latent_size, 128)\n        self.fc2 = nn.Linear(128, 4*7*7)\n        self.conv1 = nn.ConvTranspose2d(4, 32, stride=2, kernel_size=4, padding=1)\n        self.conv2 = nn.ConvTranspose2d(32, 1, stride=2, kernel_size=4, padding=1)\n        #self.cuda()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.fc2(x)\n        x = F.leaky_relu(x, 0.2)\n        x = x.view(-1, 4, 7, 7)\n        x = self.conv1(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.conv2(x)\n        x = torch.sigmoid(x)\n        return x\n\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 1)\n        #self.cuda()\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\ndef train_generative_model(encoder, generator, discriminator, dataloader):\n    generative_params = [x for x in encoder.parameters()] + [x for x in generator.parameters()]\n    gen_adam = torch.optim.Adam(generative_params, lr=.005)\n    disc_adam = torch.optim.Adam(discriminator.parameters(), lr=.02)\n    for tmp in dataloader:\n        for batch_idx, (images, labels) in enumerate(tmp):\n            disc_adam.zero_grad()\n            fake = generator(torch.randn(len(images), LATENT_SIZE))\n            disc_loss = torch.mean(F.softplus(discriminator(fake)) + F.softplus(-discriminator(images)))\n            disc_loss.backward()\n            gp_loss = calc_gradient_penalty(discriminator, images, fake)\n            gp_loss.backward()\n            disc_adam.step()\n\n            gen_adam.zero_grad()\n            mse_loss = torch.mean((generator(encoder(images)) - images) ** 2)\n            mse_loss.backward()\n            gen_loss = torch.mean(F.softplus(discriminator(images)))\n            #logger.info('Autoencoder loss: {:.03f}, Generator loss: {:.03f}, Disc. loss: {:.03f}'.format(\n            #    mse_loss, gen_loss, disc_loss))\n            gen_adam.step()\n    #print('Generative training finished')\n\n\ndef calc_gradient_penalty(discriminator, real_data, fake_data, penalty_lambda=10.0):\n    from torch import autograd\n    alpha = torch.rand(real_data.size()[0], 1, 1, 1)\n    alpha = alpha.expand(real_data.size())\n    #alpha = alpha.cuda()\n\n    # Traditional WGAN-GP\n    #interpolates = alpha * real_data + (1 - alpha) * fake_data\n    # An alternative approach\n    interpolates = torch.cat([real_data, fake_data])\n    #interpolates = interpolates.cuda()\n    interpolates = autograd.Variable(interpolates, requires_grad=True)\n\n    disc_interpolates = discriminator(interpolates)\n\n    ones = torch.ones(disc_interpolates.size())#.cuda()\n    gradients = autograd.grad(\n            outputs=disc_interpolates,\n            inputs=interpolates,\n            grad_outputs=ones,\n            create_graph=True,\n            retain_graph=True,\n            only_inputs=True)[0]\n\n    penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * penalty_lambda\n    return penalty\n\n\ndef generate_counterfactuals(encoder, generator, classifier, dataloader):\n    cf_open_set_images = []\n    for tmp in dataloader:\n        for batch_idx, (images, labels) in enumerate(tmp):\n            counterfactuals = generate_cf( encoder, generator, classifier, images)\n            cf_open_set_images.append(counterfactuals)\n            if batch_idx == 0:\n                gene = counterfactuals.numpy()\n                np.save('0.npy', gene)\n    print(\"Generated {} batches of counterfactual images\".format(len(cf_open_set_images)))\n    #imutil.show(counterfactuals, filename='example_counterfactuals.jpg', img_padding=8)\n    return cf_open_set_images\n\n\ndef generate_cf(encoder, generator, classifier, images,\n                cf_iters=1, cf_step_size=1e-2, cf_distance_weight=1.0):\n    from torch.autograd import grad\n\n    # First encode the image into latent space (z)\n    z_0 = encoder(images)\n    z = z_0.clone()\n\n    # Now perform gradient descent to update z\n    for i in range(cf_iters):\n        # Classify with one extra class\n        logits = classifier(generator(z))\n        augmented_logits = F.pad(logits, pad=(0,1))\n\n        # Use the extra class as a counterfactual target\n        batch_size, num_classes = logits.shape\n        target_tensor = torch.LongTensor(batch_size)#.cuda()\n        target_tensor[:] = num_classes\n\n        # Maximize classification probability of the counterfactual target\n        cf_loss = F.nll_loss(F.log_softmax(augmented_logits, dim=1), target_tensor)\n\n        # Regularize with distance to original z\n        distance_loss = torch.mean((z - z_0) ** 2)\n\n        # Move z toward the \"open set\" class\n        loss = cf_loss + distance_loss\n        dc_dz = grad(loss, z, loss)[0]\n        z -= cf_step_size * dc_dz\n\n        # Sanity check: Clip gradients to avoid nan in ill-conditioned inputs\n        #dc_dz = torch.clamp(dc_dz, -.1, .1)\n\n        # Optional: Normalize to the unit sphere (match encoder's settings)\n        z = norm(z)\n\n    #print(\"Generated batch of counterfactual images with cf_loss {:.03f}\".format(cf_loss))\n    # Output the generated image as an example \"unknown\" image\n    return generator(z).detach()\n\ndef train_classifier(classifier, dataloader):\n    adam = torch.optim.Adam(classifier.parameters())\n    for tmp in dataloader:\n        for batch_idx, (images, labels) in enumerate(tmp):\n            adam.zero_grad()\n            preds = F.log_softmax(classifier(images), dim=1)\n            classifier_loss = F.nll_loss(preds, labels)\n            classifier_loss.backward()\n            adam.step()\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model', type=str, default='MLP', help='neural network used in training')\n    parser.add_argument('--dataset', type=str, default='mnist', help='dataset used for training')\n    parser.add_argument('--net_config', type=lambda x: list(map(int, x.split(', '))))\n    parser.add_argument('--partition', type=str, default='homo', help='the data partitioning strategy')\n    parser.add_argument('--batch-size', type=int, default=64, help='input batch size for training (default: 64)')\n    parser.add_argument('--lr', type=float, default=0.01, help='learning rate (default: 0.01)')\n    parser.add_argument('--epochs', type=int, default=5, help='number of local epochs')\n    parser.add_argument('--n_parties', type=int, default=2,  help='number of workers in a distributed cluster')\n    parser.add_argument('--alg', type=str, default='fedavg',\n                            help='communication strategy: fedavg/fedprox')\n    parser.add_argument('--comm_round', type=int, default=50, help='number of maximum communication roun')\n    parser.add_argument('--is_same_initial', type=int, default=1, help='Whether initial all the models with the same parameters in fedavg')\n    parser.add_argument('--init_seed', type=int, default=0, help=\"Random seed\")\n    parser.add_argument('--dropout_p', type=float, required=False, default=0.0, help=\"Dropout probability. Default=0.0\")\n    parser.add_argument('--datadir', type=str, required=False, default=\"./data/\", help=\"Data directory\")\n    parser.add_argument('--reg', type=float, default=1e-5, help=\"L2 regularization strength\")\n    parser.add_argument('--logdir', type=str, required=False, default=\"./logs/\", help='Log directory path')\n    parser.add_argument('--modeldir', type=str, required=False, default=\"./models/\", help='Model directory path')\n    parser.add_argument('--beta', type=float, default=0.5, help='The parameter for the dirichlet distribution for data partitioning')\n    parser.add_argument('--device', type=str, default='cuda:0', help='The device to run the program')\n    parser.add_argument('--log_file_name', type=str, default=None, help='The log file name')\n    parser.add_argument('--optimizer', type=str, default='sgd', help='the optimizer')\n    parser.add_argument('--mu', type=float, default=1, help='the mu parameter for fedprox')\n    parser.add_argument('--noise', type=float, default=0, help='how much noise we add to some party')\n    parser.add_argument('--noise_type', type=str, default='level', help='Different level of noise or different space of noise')\n    parser.add_argument('--rho', type=float, default=0, help='Parameter controlling the momentum SGD')\n    parser.add_argument('--sample', type=float, default=1, help='Sample ratio for each communication round')\n    args = parser.parse_args()\n    return args\n\ndef init_nets(net_configs, dropout_p, n_parties, args):\n\n    nets = {net_i: None for net_i in range(n_parties)}\n\n    for net_i in range(n_parties):\n        if args.dataset == \"generated\":\n            net = PerceptronModel()\n        elif args.model == \"mlp\":\n            if args.dataset == 'covtype':\n                input_size = 54\n                output_size = 2\n                hidden_sizes = [32,16,8]\n            elif args.dataset == 'a9a':\n                input_size = 123\n                output_size = 2\n                hidden_sizes = [32,16,8]\n            elif args.dataset == 'rcv1':\n                input_size = 47236\n                output_size = 2\n                hidden_sizes = [32,16,8]\n            elif args.dataset == 'SUSY':\n                input_size = 18\n                output_size = 2\n                hidden_sizes = [16,8]\n            net = FcNet(input_size, hidden_sizes, output_size, dropout_p)\n        elif args.model == \"vgg\":\n            net = vgg11()\n        elif args.model == \"simple-cnn\":\n            if args.dataset in (\"cifar10\", \"cinic10\", \"svhn\"):\n                net = SimpleCNN(input_dim=(16 * 5 * 5), hidden_dims=[120, 84], output_dim=11)\n            elif args.dataset in (\"cifar100\"):\n                net = SimpleCNN(input_dim=(16 * 5 * 5), hidden_dims=[120, 84], output_dim=101)\n            elif args.dataset in (\"mnist\", 'femnist', 'fmnist'):\n                net = SimpleCNNMNIST(input_dim=(16 * 4 * 4), hidden_dims=[120, 84], output_dim=11)\n            elif args.dataset == 'celeba':\n                net = SimpleCNN(input_dim=(16 * 5 * 5), hidden_dims=[120, 84], output_dim=2)\n        elif args.model == \"vgg-9\":\n            if args.dataset in (\"mnist\", 'femnist'):\n                net = ModerateCNNMNIST()\n            elif args.dataset in (\"cifar10\", \"cinic10\", \"svhn\"):\n                # print(\"in moderate cnn\")\n                net = ModerateCNN()\n            elif args.dataset == 'celeba':\n                net = ModerateCNN(output_dim=2)\n        elif args.model == \"resnet50\":\n            if args.dataset == \"cifar100\":\n                net = ResNet50_cifar10(num_classes=101)\n            elif args.dataset == \"tinyimagenet\":\n                net = ResNet50_cifar10(num_classes=201)\n            else:\n                net = ResNet50_cifar10(num_classes=11)\n        elif args.model == \"resnet18\":\n            if args.dataset == \"cifar100\":\n                net = ResNet18_cifar10(num_classes=101)\n            elif args.dataset == \"tinyimagenet\":\n                net = ResNet18_cifar10(num_classes=201)\n            else:\n                net = ResNet18_cifar10(num_classes=11)\n            print(\"resnet18\")\n        elif args.model == \"vgg16\":\n            net = vgg16()\n        else:\n            print(\"not supported yet\")\n            exit(1)\n        nets[net_i] = net\n\n    model_meta_data = []\n    layer_type = []\n    for (k, v) in nets[0].state_dict().items():\n        model_meta_data.append(v.shape)\n        layer_type.append(k)\n\n    return nets, model_meta_data, layer_type\n\nop = transforms.RandomChoice( [\n    #transforms.RandomResizedCrop(sz),\n    transforms.RandomRotation(degrees=(15,75)),\n    transforms.RandomRotation(degrees=(-75,-15)),\n    transforms.RandomRotation(degrees=(85,90)),\n    transforms.RandomRotation(degrees=(-90,-85)),\n    transforms.RandomRotation(degrees=(175,180)),\n    #transforms.RandomAffine(0,translate=(0.2,0.2)),\n    #transforms.RandomPerspective(distortion_scale=1,p=1),\n    #transforms.RandomHorizontalFlip(p=1),\n    #transforms.RandomVerticalFlip(p=1)\n])\n\ndef cut(x):\n    x_gen = copy.deepcopy(x.cpu().numpy())\n    half = int(x_gen.shape[2] / 2)\n    rnd = random.randint(0,5)\n    pl = random.randint(0,half-1)\n    pl2 = random.randint(0,half-1)\n    while (abs(pl-pl2)<half/2):\n        pl2 = random.randint(0,half-1)\n    if rnd <= 1:\n        x_gen[:,:,pl:pl+half] = x_gen[:,:,pl2:pl2+half]\n    elif rnd == 2:\n        x_gen[:,:,half:] = x_gen[:,:,:half]\n        x_gen[:,:,:half] = copy.deepcopy(x.cpu().numpy())[:,:,half:]\n    elif rnd <= 4:\n        x_gen[:,pl:pl+half,:] = x_gen[:,pl2:pl2+half,:]\n    else:\n        x_gen[:,half:,:] = x_gen[:,:half,:]\n        x_gen[:,:half,:] = copy.deepcopy(x.cpu().numpy())[:,half:,:]\n    x_gen = torch.Tensor(x_gen)\n\n    return x_gen\n\ndef rot(x):\n    #rnd = random.randint(0,20)\n    #if rnd < 21:\n    x_gen = copy.deepcopy(x.cpu().numpy())\n    half = int(x_gen.shape[2] / 2)\n    pl = random.randint(0,half-1)\n    rnd = random.randint(1,3)\n\n    x_gen[:,pl:pl+half,half:] = np.rot90(x_gen[:,pl:pl+half,half:],k=rnd,axes=(1,2))\n    x_gen[:,pl:pl+half,:half] = np.rot90(x_gen[:,pl:pl+half,:half],k=rnd,axes=(1,2))\n    x_gen = torch.Tensor(x_gen)\n    #else:\n    #    x_gen = op(copy.deepcopy(x))\n    #    if rnd < 20:\n    #        x_gen = torch.max(x_gen, x)\n    #    else:\n    #        x_gen = torch.min(x_gen, x)\n\n    return x_gen\n\ndef paint(x):\n    x_gen = copy.deepcopy(x.cpu().numpy())\n    size = int(x_gen.shape[2])\n    sq = 4\n    pl = random.randint(sq,size-sq*2)\n    pl2 = random.randint(sq,size-sq-1)\n    rnd = random.randint(0,1)\n    if rnd == 0:\n        for i in range(sq,size-sq):\n            x_gen[:,i,pl:pl+sq] = x_gen[:,pl2,pl:pl+sq]\n    elif rnd == 1:\n        for i in range(sq,size-sq):\n            x_gen[:,pl:pl+sq,i] = x_gen[:,pl:pl+sq,pl2]\n    x_gen = torch.Tensor(x_gen)\n\n    return x_gen\n\ndef blur(x):\n    rnd = random.randint(0,1)\n    sz = random.randint(1,4)*2+1\n    sz2 = random.randint(0,2)*2+1\n    if rnd == 0:\n        func = transforms.GaussianBlur(kernel_size=(sz, sz2), sigma=(10, 100))\n    else:\n        func = transforms.GaussianBlur(kernel_size=(sz2, sz), sigma=(10, 100))\n    \n    return func(x)\n\ndef shuffle(x):\n    rnd = random.randint(0,1)\n    x_gen = copy.deepcopy(x.cpu().numpy())\n    sz = x_gen.shape[0]\n    li = np.split(x_gen, range(1,sz,10), axis=rnd)\n    np.random.shuffle(li)\n    t = np.concatenate(li, axis=rnd)\n    t = torch.Tensor(t)\n    return t\n\ndef train_net_vote(net_id, net, train_dataloader, test_dataloader, epochs, lr, args_optimizer, sz, num_class=10, device=\"cpu\"):\n    logger.info('Training network %s' % str(net_id))\n    \n    if type(train_dataloader) == type([1]):\n        pass\n    else:\n        train_dataloader = [train_dataloader]\n    '''\n    classifier = Classifier(num_classes=NUM_CLASSES).to(device)\n    for i in range(CLASSIFIER_EPOCHS):\n        train_classifier(classifier, train_dataloader)\n\n    encoder = Encoder(latent_size=LATENT_SIZE).to(device)\n    generator = Generator(latent_size=LATENT_SIZE).to(device)\n    discriminator = Discriminator().to(device)\n    for i in range(GENERATIVE_EPOCHS):\n        train_generative_model(encoder, generator, discriminator, train_dataloader)\n    open_set_images = generate_counterfactuals(encoder, generator, classifier, train_dataloader)\n    '''\n\n    #if args_optimizer == 'adam':\n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr, weight_decay=args.reg)\n    #elif args_optimizer == 'amsgrad':\n    #    optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr, weight_decay=args.reg,\n    #                           amsgrad=True)\n    #elif args_optimizer == 'sgd':\n    #    optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=lr, momentum=args.rho, weight_decay=args.reg)\n    #device = 'cuda:7'\n    criterion = nn.CrossEntropyLoss(ignore_index=11).to(device)\n    #c2 = nn.CrossEntropyLoss(reduction='none').to(device)\n    net.to(device)\n\n    cnt = 0\n    rnd = []\n\n    toImg = transforms.ToPILImage()\n    toTensor = transforms.ToTensor()\n\n    op = transforms.RandomChoice( [\n        #transforms.RandomResizedCrop(sz),\n        transforms.RandomRotation(degrees=(15,75)),\n        transforms.RandomRotation(degrees=(-75,-15)),\n        transforms.RandomRotation(degrees=(85,90)),\n        transforms.RandomRotation(degrees=(-90,-85)),\n        transforms.RandomRotation(degrees=(175,180)),\n        #transforms.RandomAffine(0,translate=(0.2,0.2)),\n        #transforms.RandomPerspective(distortion_scale=1,p=1),\n        #transforms.RandomHorizontalFlip(p=1),\n        #transforms.RandomVerticalFlip(p=1)\n    ])\n\n    aug = transforms.Compose([\n        toImg,\n        op,\n        toTensor\n    ])\n\n    aug_crop =  transforms.RandomChoice( [\n        transforms.RandomResizedCrop(sz, scale=(0.1, 0.33)), # good\n        transforms.Lambda(lambda img: blur(img)), # good\n        #transforms.Lambda(lambda img: shuffle(img)), # bad\n        transforms.RandomErasing(p=1, scale=(0.33, 0.5)), # good\n        transforms.Lambda(lambda img: cut(img)), # fine\n        transforms.Lambda(lambda img: rot(img)),\n        transforms.Lambda(lambda img: cut(img)),\n        transforms.Lambda(lambda img: rot(img)),\n        #transforms.Lambda(lambda img: paint(img))\n    ])\n\n    attack = FastGradientSignUntargeted(net, \n                                        epsilon=0.5, \n                                        alpha=0.002, \n                                        min_val=0, \n                                        max_val=1, \n                                        max_iters=5,\n                                        device=device)\n\n    cp = CutPasteUnion()\n\n    aug_final = transforms.RandomChoice( [\n        transforms.Lambda(lambda img: aug_crop(img)),\n        #transforms.Lambda(lambda img: cp(img)) # delete this comment if you want to add cutpaste augmentation\n    ])\n    print(f\"{num_class} : num_class\")\n\n    for epoch in range(epochs):\n        epoch_loss_collector = []\n        for tmp in train_dataloader:\n\n            for batch_idx, (images, targets) in enumerate(tmp):\n               \n                optimizer.zero_grad()\n\n                B,C,H,W = images.shape\n                \n                images, targets = images.to(device), targets.to(device)\n                y_gen = torch.full((B,), num_class, dtype=torch.long, device=device)\n\n                x_gen11 = [aug_final(img.cpu()) for img in images]\n                x_gen11 = torch.stack(x_gen11).to(device)\n\n                # adv_data = attack.perturb(x_gen11, y_gen)\n               \n                optimizer.zero_grad()\n                # combined_batch = torch.cat([images, x_gen11, adv_data], dim=0)\n                combined_batch = torch.cat([images, x_gen11], dim=0)\n\n                combined_out, combined_mid = net(combined_batch)\n                out, mid = combined_out[:B], combined_mid[:B]\n                out_gen11 = combined_out[B:]\n                # out_gen11 = combined_out[B:2*B]\n                # out_adv = combined_out[2*B:]\n                \n                one_hot = torch.zeros(B, num_class+1, device=device)\n                one_hot.scatter_(1, targets.reshape(-1, 1), 1)\n                out_second = out - one_hot * 10000\n\n                ind = torch.randperm(targets.size(0), device=device)\n                y_mask = torch.where(targets == targets[ind], targets, torch.tensor(10, device=device))\n\n                phi=torch.distributions.beta.Beta(1, 1).sample([]).item()\n                mixed_embeddings = phi * mid + (1-phi) * mid[ind]\n                mixed_out = net.later_layers(mixed_embeddings.to(device)) \n\n                \n                def anneal(start, end, epoch, total):\n                    return start - (start - end) * (min(epoch, total) / total)\n                \n                alpha = anneal(1.0, 0.1, epoch, epochs)  \n                delta = 0.0\n             \n                beta = 1 \n                gamma = 0.01\n           \n                loss = criterion(out, targets) + alpha*criterion(out_gen11, y_gen) + beta*criterion(out_second, y_gen) + gamma*criterion(mixed_out, y_mask)# + delta*criterion(out_adv, y_gen)\n                loss.backward()\n                optimizer.step()\n\n                cnt += 1\n                epoch_loss_collector.append(loss.item())\n\n        epoch_loss = sum(epoch_loss_collector) / len(epoch_loss_collector)\n        #auc = compute_auc_outlier_detection(net_id, net, test_dataloader, device=device) #can be used to perform traditional outlier detection experiments, calculate ROC-AUC under noniid-#label1 partition\n        logger.info('Epoch: %d Loss: %f' % (epoch, epoch_loss))\n\n        #train_acc = compute_accuracy(net, train_dataloader, device=device)\n        #test_acc, conf_matrix = compute_accuracy(net, test_dataloader, get_confusion_matrix=True, device=device)\n\n        #writer.add_scalar('Accuracy/train', train_acc, epoch)\n        #writer.add_scalar('Accuracy/test', test_acc, epoch)\n\n        if epoch % 5 == 0:\n            train_acc = compute_accuracy(net, train_dataloader, device=device)\n            test_acc = compute_accuracy(net, test_dataloader, device=device)\n        \n            logger.info('>> Training accuracy: %f' % train_acc)\n            logger.info('>> Test accuracy: %f' % test_acc)\n\n    #device = 'cpu'\n    #net.to(device)\n    '''\n    flag = False\n    for tmp in train_dataloader:\n        for batch_idx, (x, target) in enumerate(tmp):\n            x_gen11 = copy.deepcopy(x.cpu().numpy())\n            for i in range(x_gen11.shape[0]):\n                x_gen11[i] = aug_crop(torch.Tensor(x_gen11[i]))\n            x_gen11 = torch.Tensor(x_gen11).to(device)\n\n            out, mid = net(x_gen11)\n\n            if not flag:\n                flag = True\n                outliers = mid.cpu().detach().numpy()\n            else:\n                outliers = np.concatenate((outliers,mid.cpu().detach().numpy()))\n    '''\n    train_acc, threshold, max_prob, avg_max = compute_accuracy(net, train_dataloader, calc=True, device=device)\n    test_acc = compute_accuracy(net, test_dataloader, device=device)#, add=outliers)\n    \n    logger.info(threshold)\n    logger.info(max_prob)\n    logger.info(avg_max)\n\n    logger.info('>> Training accuracy: %f' % train_acc)\n    logger.info('>> Test accuracy: %f' % test_acc)\n\n    logger.info(' ** Training complete **')\n    return threshold, max_prob, avg_max\n\n\ndef local_train_net_vote(nets, selected, args, net_dataidx_map, test_dl = None, device=\"cpu\"):\n    threshold_list = []\n\n    for net_id, net in nets.items():\n        if net_id not in selected:\n            continue\n        dataidxs = net_dataidx_map[net_id]\n\n        logger.info(\"Training network %s. n_training: %d\" % (str(net_id), len(dataidxs)))\n        # move the model to cuda device:\n        net.to(device)\n\n        noise_level = args.noise\n        if net_id == args.n_parties - 1:\n            noise_level = 0\n\n        if args.noise_type == 'space':\n            train_dl_local, test_dl_local, _, _ = get_dataloader(args.dataset, args.datadir, args.batch_size, 32, dataidxs, noise_level, net_id, args.n_parties-1)\n        else:\n            noise_level = args.noise / (args.n_parties - 1) * net_id\n            train_dl_local, test_dl_local, _, _ = get_dataloader(args.dataset, args.datadir, args.batch_size, 32, dataidxs, noise_level)\n        train_dl_global, test_dl_global, _, _ = get_dataloader(args.dataset, args.datadir, args.batch_size, 32)\n        n_epoch = args.epochs\n\n        if args.dataset in ('mnist', 'fmnist'):\n            sz = 28\n        else:\n            sz = 32\n        \n        num_class = 10\n        if args.dataset == 'cifar100':\n            num_class = 100\n        elif args.dataset == 'tinyimagenet':\n            num_class = 200\n        \n        threshold, max_prob, avg_max = train_net_vote(net_id, net, train_dl_local, test_dl, n_epoch, args.lr, args.optimizer, sz, num_class=num_class, device=device)\n        threshold_list.append([float(threshold), float(max_prob), float(avg_max)])\n       \n    return threshold_list\n\n\nif __name__ == '__main__':\n    # torch.set_printoptions(profile=\"full\")\n    args = get_args()\n    mkdirs(args.logdir)\n    mkdirs(args.modeldir)\n    # if args.log_file_name is None:\n    #     argument_path='experiment_arguments-%s.json' % datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M-%S\")\n    # else:\n    #     argument_path=args.log_file_name+'.json'\n    # with open(os.path.join(args.logdir, argument_path), 'w') as f:\n    #     json.dump(str(args), f)\n    device = torch.device(args.device)\n    # logging.basicConfig(filename='test.log', level=logger.info, filemode='w')\n    # logging.info(\"test\")\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n\n    # if args.log_file_name is None:\n    #     args.log_file_name = 'experiment_log-%s' % (datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M-%S\"))\n    # log_path=args.log_file_name+'.log'\n    logging.basicConfig(\n        # filename='/home/qinbin/test.log',\n        format='%(asctime)s %(levelname)-8s %(message)s',\n        datefmt='%m-%d %H:%M', level=logging.DEBUG, filemode='w')\n\n    logger = logging.getLogger()\n    logger.setLevel(logging.DEBUG)\n    logger.info(device)\n\n    seed = args.init_seed\n    logger.info(\"#\" * 100)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    logger.info(\"Partitioning data\")\n    X_train, y_train, X_test, y_test, net_dataidx_map, traindata_cls_counts = partition_data(\n        args.dataset, args.datadir, args.logdir, args.partition, args.n_parties, beta=args.beta)\n\n    n_classes = len(np.unique(y_train))\n\n    train_dl_global, test_dl_global, train_ds_global, test_ds_global = get_dataloader(args.dataset,\n                                                                                        args.datadir,\n                                                                                        args.batch_size,\n                                                                                        32)\n\n    print(\"len train_dl_global:\", len(train_ds_global))\n\n\n    data_size = len(test_ds_global)\n\n    # test_dl = data.DataLoader(dataset=test_ds_global, batch_size=32, shuffle=False)\n\n    train_all_in_list = []\n    test_all_in_list = []\n    if args.noise > 0:\n        for party_id in range(args.n_parties):\n            dataidxs = net_dataidx_map[party_id]\n\n            noise_level = args.noise\n            if party_id == args.n_parties - 1:\n                noise_level = 0\n\n            if args.noise_type == 'space':\n                train_dl_local, test_dl_local, train_ds_local, test_ds_local = get_dataloader(args.dataset, args.datadir, args.batch_size, 32, dataidxs, noise_level, party_id, args.n_parties-1)\n            else:\n                noise_level = args.noise / (args.n_parties - 1) * party_id\n                train_dl_local, test_dl_local, train_ds_local, test_ds_local = get_dataloader(args.dataset, args.datadir, args.batch_size, 32, dataidxs, noise_level)\n            train_all_in_list.append(train_ds_local)\n            test_all_in_list.append(test_ds_local)\n        train_all_in_ds = data.ConcatDataset(train_all_in_list)\n        train_dl_global = data.DataLoader(dataset=train_all_in_ds, batch_size=args.batch_size, shuffle=True)\n        test_all_in_ds = data.ConcatDataset(test_all_in_list)\n        test_dl_global = data.DataLoader(dataset=test_all_in_ds, batch_size=32, shuffle=False)\n\n\n    if args.alg == 'vote':\n        logger.info(\"Initializing nets\")\n        nets, local_model_meta_data, layer_type = init_nets(args.net_config, args.dropout_p, args.n_parties, args)\n        arr = np.arange(args.n_parties)\n        threshold_list=[]\n        threshold_list = local_train_net_vote(nets, arr, args, net_dataidx_map, test_dl = test_dl_global, device=device)\n        #logger.info(threshold_list)\n\n        model_list = [net for net_id, net in nets.items()]\n        \n        #train_acc = compute_accuracy_vote(nets, train_dl_global)\n        for factor in [1]:\n            logger.info(\"Factor = {}\".format(factor))\n            #logger.info(\"Normalize\")\n            #for accepted_vote in range(1, 11):\n            #    test_acc = compute_accuracy_vote(model_list, threshold_list, test_dl_global, accepted_vote, factor=factor,device=device)\n            #    logger.info(\"Max {} vote: test acc = {}\".format(accepted_vote, test_acc))\n            \n            logger.info(\"Not Normalize\")\n            for accepted_vote in range(1, 11):\n                test_acc, half, pred_labels_list = compute_accuracy_vote_soft(model_list, threshold_list, test_dl_global, accepted_vote, normalize = False, factor=factor,device=device)\n                logger.info(\"Max {} vote: test acc = {}\".format(accepted_vote, test_acc))\n            #logger.info(half)\n            #logger.info(pred_labels_list.shape)\n            #logger.info(pred_labels_list)\n\n        stu_nets = init_nets(args.net_config, args.dropout_p, 1, args)\n        stu_model = stu_nets[0][0]\n        distill_soft(stu_model, pred_labels_list, test_dl_global, half, args=args, device=device)\n        # compute_accuracy_vote_soft() and distill_soft() for soft label distillation like FedDF. \n        # compute_accuracy_vote() and distill() are hard label distillation.\n        # Soft label is usually better, especially for complicated datasets like CIFAR-10, CIFAR-100.\n            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T10:06:38.004385Z","iopub.execute_input":"2025-12-28T10:06:38.004752Z","iopub.status.idle":"2025-12-28T10:06:38.022191Z","shell.execute_reply.started":"2025-12-28T10:06:38.004721Z","shell.execute_reply":"2025-12-28T10:06:38.021540Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Overwriting experiments.py\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!python experiments.py --model=resnet18 \\\n\t--dataset=cifar10 \\\n\t--alg=vote \\\n\t--lr=5e-4 \\\n\t--batch-size=64 \\\n\t--epochs=100 \\\n\t--n_parties=5 \\\n\t--rho=0.9 \\\n\t--comm_round=1 \\\n\t--partition=noniid-labeldir \\\n\t--beta=0.5\\\n\t--device='cuda:0'\\\n\t--datadir='./data/' \\\n\t--init_seed=0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T10:06:38.679428Z","iopub.execute_input":"2025-12-28T10:06:38.680058Z","iopub.status.idle":"2025-12-28T10:18:03.362304Z","shell.execute_reply.started":"2025-12-28T10:06:38.680028Z","shell.execute_reply":"2025-12-28T10:18:03.361591Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"12-28 10:06 INFO     cuda:0\n12-28 10:06 INFO     ####################################################################################################\n12-28 10:06 INFO     Partitioning data\n12-28 10:06 INFO     Data statistics: {0: {np.int64(0): np.int64(2538), np.int64(1): np.int64(2206), np.int64(2): np.int64(409), np.int64(3): np.int64(3154), np.int64(4): np.int64(40), np.int64(5): np.int64(178), np.int64(6): np.int64(387), np.int64(7): np.int64(128), np.int64(8): np.int64(2363)}, 1: {np.int64(0): np.int64(43), np.int64(1): np.int64(215), np.int64(2): np.int64(202), np.int64(3): np.int64(1423), np.int64(4): np.int64(1777), np.int64(5): np.int64(2276), np.int64(6): np.int64(1), np.int64(7): np.int64(498), np.int64(8): np.int64(446), np.int64(9): np.int64(878)}, 2: {np.int64(0): np.int64(23), np.int64(1): np.int64(1517), np.int64(2): np.int64(1244), np.int64(3): np.int64(192), np.int64(4): np.int64(2349), np.int64(5): np.int64(208), np.int64(6): np.int64(259), np.int64(7): np.int64(2175), np.int64(8): np.int64(682), np.int64(9): np.int64(114)}, 3: {np.int64(0): np.int64(1053), np.int64(1): np.int64(1004), np.int64(2): np.int64(2473), np.int64(4): np.int64(108), np.int64(5): np.int64(512), np.int64(6): np.int64(897), np.int64(7): np.int64(1057), np.int64(8): np.int64(1368), np.int64(9): np.int64(2583)}, 4: {np.int64(0): np.int64(1343), np.int64(1): np.int64(58), np.int64(2): np.int64(672), np.int64(3): np.int64(231), np.int64(4): np.int64(726), np.int64(5): np.int64(1826), np.int64(6): np.int64(3456), np.int64(7): np.int64(1142), np.int64(8): np.int64(141), np.int64(9): np.int64(1425)}}\nlen train_dl_global: 50000\n12-28 10:06 INFO     Initializing nets\nresnet18\nresnet18\nresnet18\nresnet18\nresnet18\n12-28 10:06 INFO     Training network 0. n_training: 11403\n12-28 10:06 INFO     Training network 0\n10 : num_class\n12-28 10:07 INFO     Epoch: 0 Loss: 2.613930\n12-28 10:07 INFO     >> Training accuracy: 0.410944\n12-28 10:07 INFO     >> Test accuracy: 0.206100\n12-28 10:07 INFO     1\n12-28 10:07 INFO     1\n12-28 10:07 INFO     1\n12-28 10:07 INFO     >> Training accuracy: 0.403929\n12-28 10:07 INFO     >> Test accuracy: 0.206100\n12-28 10:07 INFO      ** Training complete **\n12-28 10:07 INFO     Training network 1. n_training: 7759\n12-28 10:07 INFO     Training network 1\n10 : num_class\n12-28 10:07 INFO     Epoch: 0 Loss: 3.102800\n12-28 10:07 INFO     >> Training accuracy: 0.055420\n12-28 10:07 INFO     >> Test accuracy: 0.085800\n12-28 10:08 INFO     1\n12-28 10:08 INFO     1\n12-28 10:08 INFO     1\n12-28 10:08 INFO     >> Training accuracy: 0.055033\n12-28 10:08 INFO     >> Test accuracy: 0.085800\n12-28 10:08 INFO      ** Training complete **\n12-28 10:08 INFO     Training network 2. n_training: 8763\n12-28 10:08 INFO     Training network 2\n10 : num_class\n12-28 10:08 INFO     Epoch: 0 Loss: 3.074478\n12-28 10:08 INFO     >> Training accuracy: 0.154970\n12-28 10:08 INFO     >> Test accuracy: 0.091400\n12-28 10:08 INFO     1\n12-28 10:08 INFO     1\n12-28 10:08 INFO     1\n12-28 10:08 INFO     >> Training accuracy: 0.157594\n12-28 10:08 INFO     >> Test accuracy: 0.091400\n12-28 10:08 INFO      ** Training complete **\n12-28 10:08 INFO     Training network 3. n_training: 11055\n12-28 10:08 INFO     Training network 3\n10 : num_class\n12-28 10:08 INFO     Epoch: 0 Loss: 3.030645\n12-28 10:09 INFO     >> Training accuracy: 0.057892\n12-28 10:09 INFO     >> Test accuracy: 0.063800\n12-28 10:09 INFO     1\n12-28 10:09 INFO     1\n12-28 10:09 INFO     1\n12-28 10:09 INFO     >> Training accuracy: 0.057892\n12-28 10:09 INFO     >> Test accuracy: 0.063800\n12-28 10:09 INFO      ** Training complete **\n12-28 10:09 INFO     Training network 4. n_training: 11020\n12-28 10:09 INFO     Training network 4\n10 : num_class\n12-28 10:09 INFO     Epoch: 0 Loss: 2.941234\n12-28 10:09 INFO     >> Training accuracy: 0.453993\n12-28 10:09 INFO     >> Test accuracy: 0.242700\n12-28 10:09 INFO     1\n12-28 10:09 INFO     1\n12-28 10:09 INFO     1\n12-28 10:09 INFO     >> Training accuracy: 0.451996\n12-28 10:09 INFO     >> Test accuracy: 0.242700\n12-28 10:09 INFO      ** Training complete **\n12-28 10:09 INFO     Factor = 1\n12-28 10:09 INFO     Not Normalize\n12-28 10:10 INFO     Max 1 vote: test acc = 0.4353\n12-28 10:10 INFO     Max 2 vote: test acc = 0.4553\n12-28 10:10 INFO     Max 3 vote: test acc = 0.4827\n12-28 10:10 INFO     Max 4 vote: test acc = 0.4948\n12-28 10:11 INFO     Max 5 vote: test acc = 0.5087\n12-28 10:11 INFO     Max 6 vote: test acc = 0.5087\n12-28 10:11 INFO     Max 7 vote: test acc = 0.5087\n12-28 10:11 INFO     Max 8 vote: test acc = 0.5087\n12-28 10:11 INFO     Max 9 vote: test acc = 0.5087\n12-28 10:12 INFO     Max 10 vote: test acc = 0.5087\nresnet18\n/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:3359: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n  warnings.warn(\n12-28 10:12 INFO     Epoch: 0 Loss: 0.022007\n12-28 10:12 INFO     Epoch: 1 Loss: 0.010987\n12-28 10:12 INFO     Epoch: 2 Loss: 0.007968\n12-28 10:12 INFO     Epoch: 3 Loss: 0.007328\n12-28 10:12 INFO     Epoch: 4 Loss: 0.007259\n12-28 10:12 INFO     Epoch: 5 Loss: 0.007549\n12-28 10:12 INFO     Epoch: 6 Loss: 0.006778\n12-28 10:12 INFO     Epoch: 7 Loss: 0.005796\n12-28 10:12 INFO     Epoch: 8 Loss: 0.004790\n12-28 10:12 INFO     Epoch: 9 Loss: 0.004604\n12-28 10:12 INFO     Epoch: 10 Loss: 0.004624\n12-28 10:12 INFO     Epoch: 11 Loss: 0.004836\n12-28 10:12 INFO     Epoch: 12 Loss: 0.005073\n12-28 10:12 INFO     Epoch: 13 Loss: 0.004973\n12-28 10:13 INFO     Epoch: 14 Loss: 0.004642\n12-28 10:13 INFO     Epoch: 15 Loss: 0.004009\n12-28 10:13 INFO     Epoch: 16 Loss: 0.003855\n12-28 10:13 INFO     Epoch: 17 Loss: 0.004170\n12-28 10:13 INFO     Epoch: 18 Loss: 0.004249\n12-28 10:13 INFO     Epoch: 19 Loss: 0.004067\n12-28 10:13 INFO     Epoch: 20 Loss: 0.003753\n12-28 10:13 INFO     Epoch: 21 Loss: 0.003760\n12-28 10:13 INFO     Epoch: 22 Loss: 0.003845\n12-28 10:13 INFO     Epoch: 23 Loss: 0.003929\n12-28 10:13 INFO     Epoch: 24 Loss: 0.003777\n12-28 10:13 INFO     Epoch: 25 Loss: 0.003483\n12-28 10:13 INFO     Epoch: 26 Loss: 0.003362\n12-28 10:13 INFO     Epoch: 27 Loss: 0.003409\n12-28 10:13 INFO     Epoch: 28 Loss: 0.003478\n12-28 10:13 INFO     Epoch: 29 Loss: 0.003437\n12-28 10:13 INFO     Epoch: 30 Loss: 0.003515\n12-28 10:14 INFO     Epoch: 31 Loss: 0.003262\n12-28 10:14 INFO     Epoch: 32 Loss: 0.003268\n12-28 10:14 INFO     Epoch: 33 Loss: 0.003318\n12-28 10:14 INFO     Epoch: 34 Loss: 0.003190\n12-28 10:14 INFO     Epoch: 35 Loss: 0.003037\n12-28 10:14 INFO     Epoch: 36 Loss: 0.003013\n12-28 10:14 INFO     Epoch: 37 Loss: 0.003231\n12-28 10:14 INFO     Epoch: 38 Loss: 0.003119\n12-28 10:14 INFO     Epoch: 39 Loss: 0.003069\n12-28 10:14 INFO     Epoch: 40 Loss: 0.002860\n12-28 10:14 INFO     Epoch: 41 Loss: 0.002850\n12-28 10:14 INFO     Epoch: 42 Loss: 0.002889\n12-28 10:14 INFO     Epoch: 43 Loss: 0.002755\n12-28 10:14 INFO     Epoch: 44 Loss: 0.002607\n12-28 10:14 INFO     Epoch: 45 Loss: 0.002670\n12-28 10:14 INFO     Epoch: 46 Loss: 0.002700\n12-28 10:14 INFO     Epoch: 47 Loss: 0.002745\n12-28 10:15 INFO     Epoch: 48 Loss: 0.002591\n12-28 10:15 INFO     Epoch: 49 Loss: 0.002701\n12-28 10:15 INFO     Epoch: 50 Loss: 0.002673\n12-28 10:15 INFO     Epoch: 51 Loss: 0.002476\n12-28 10:15 INFO     Epoch: 52 Loss: 0.002313\n12-28 10:15 INFO     Epoch: 53 Loss: 0.002253\n12-28 10:15 INFO     Epoch: 54 Loss: 0.002309\n12-28 10:15 INFO     Epoch: 55 Loss: 0.002406\n12-28 10:15 INFO     Epoch: 56 Loss: 0.002357\n12-28 10:15 INFO     Epoch: 57 Loss: 0.002373\n12-28 10:15 INFO     Epoch: 58 Loss: 0.002440\n12-28 10:15 INFO     Epoch: 59 Loss: 0.002367\n12-28 10:15 INFO     Epoch: 60 Loss: 0.002435\n12-28 10:15 INFO     Epoch: 61 Loss: 0.002254\n12-28 10:15 INFO     Epoch: 62 Loss: 0.002196\n12-28 10:15 INFO     Epoch: 63 Loss: 0.002128\n12-28 10:15 INFO     Epoch: 64 Loss: 0.002192\n12-28 10:16 INFO     Epoch: 65 Loss: 0.002190\n12-28 10:16 INFO     Epoch: 66 Loss: 0.002095\n12-28 10:16 INFO     Epoch: 67 Loss: 0.002070\n12-28 10:16 INFO     Epoch: 68 Loss: 0.002074\n12-28 10:16 INFO     Epoch: 69 Loss: 0.002168\n12-28 10:16 INFO     Epoch: 70 Loss: 0.002158\n12-28 10:16 INFO     Epoch: 71 Loss: 0.002032\n12-28 10:16 INFO     Epoch: 72 Loss: 0.001990\n12-28 10:16 INFO     Epoch: 73 Loss: 0.001963\n12-28 10:16 INFO     Epoch: 74 Loss: 0.002069\n12-28 10:16 INFO     Epoch: 75 Loss: 0.002059\n12-28 10:16 INFO     Epoch: 76 Loss: 0.002062\n12-28 10:16 INFO     Epoch: 77 Loss: 0.002091\n12-28 10:16 INFO     Epoch: 78 Loss: 0.002076\n12-28 10:16 INFO     Epoch: 79 Loss: 0.001970\n12-28 10:16 INFO     Epoch: 80 Loss: 0.001735\n12-28 10:16 INFO     Epoch: 81 Loss: 0.001675\n12-28 10:17 INFO     Epoch: 82 Loss: 0.001770\n12-28 10:17 INFO     Epoch: 83 Loss: 0.001849\n12-28 10:17 INFO     Epoch: 84 Loss: 0.001843\n12-28 10:17 INFO     Epoch: 85 Loss: 0.001927\n12-28 10:17 INFO     Epoch: 86 Loss: 0.001935\n12-28 10:17 INFO     Epoch: 87 Loss: 0.001925\n12-28 10:17 INFO     Epoch: 88 Loss: 0.001885\n12-28 10:17 INFO     Epoch: 89 Loss: 0.001837\n12-28 10:17 INFO     Epoch: 90 Loss: 0.001750\n12-28 10:17 INFO     Epoch: 91 Loss: 0.001687\n12-28 10:17 INFO     Epoch: 92 Loss: 0.001713\n12-28 10:17 INFO     Epoch: 93 Loss: 0.001805\n12-28 10:17 INFO     Epoch: 94 Loss: 0.001796\n12-28 10:17 INFO     Epoch: 95 Loss: 0.001778\n12-28 10:17 INFO     Epoch: 96 Loss: 0.001827\n12-28 10:17 INFO     Epoch: 97 Loss: 0.001765\n12-28 10:17 INFO     Epoch: 98 Loss: 0.001612\n12-28 10:18 INFO     Epoch: 99 Loss: 0.001598\n12-28 10:18 INFO     0.46565495207667734\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}